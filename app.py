"""Streamlit dashboard for exploring Adult Income classifiers.

The app loads trained model pipelines from `model/` and lets you:
- compare benchmark scores,
- run batch predictions on uploaded CSVs,
- evaluate when ground-truth labels are present,
- try a single manual example.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Mapping

import joblib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import streamlit as st
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    roc_auc_score,
)


BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
MODEL_DIR = BASE_DIR / "model"
APP_TITLE = "Adult Income Classifier"


def apply_global_styles() -> None:
    """Apply lightweight CSS for a cleaner, consistent look."""
    st.markdown(
        """
<style>
  .block-container { padding-top: 1.2rem; padding-bottom: 2.5rem; }
  h1, h2, h3 { letter-spacing: -0.015em; }
  .muted { color: rgba(250, 250, 250, 0.72); }
  .card {
    border: 1px solid rgba(255,255,255,0.10);
    border-radius: 14px;
    padding: 14px 16px;
    background: rgba(255,255,255,0.03);
  }
  .tiny { font-size: 0.9rem; opacity: 0.9; }
  [data-testid="stFileUploader"] section { padding: 0.6rem 0.75rem; }
</style>
""",
        unsafe_allow_html=True,
    )


@st.cache_resource
def load_metadata() -> Mapping[str, Any]:
    """Load model/dataset metadata generated by `train_models.py`."""
    metadata_path = MODEL_DIR / "metadata.json"
    with metadata_path.open("r", encoding="utf-8") as file:
        return json.load(file)


@st.cache_resource
def load_all_models() -> dict[str, object]:
    """Load persisted model pipelines from disk."""
    models: dict[str, object] = {}
    for model_file in MODEL_DIR.glob("*_pipeline.joblib"):
        display_name = model_file.stem.replace("_pipeline", "").replace("_", " ").title()
        models[display_name] = joblib.load(model_file)
    return models


def compute_metrics(
    y_true: pd.Series,
    y_pred: pd.Series,
    y_prob_positive: pd.Series,
    positive_label: str,
) -> dict[str, float]:
    """Compute evaluation metrics for binary classification."""
    y_binary = (y_true == positive_label).astype(int)
    metrics = {
        "Accuracy": accuracy_score(y_true, y_pred),
        "AUC": roc_auc_score(y_binary, y_prob_positive),
        "Precision": precision_score(y_true, y_pred, pos_label=positive_label),
        "Recall": recall_score(y_true, y_pred, pos_label=positive_label),
        "F1 Score": f1_score(y_true, y_pred, pos_label=positive_label),
        "MCC": matthews_corrcoef(y_true, y_pred),
    }
    return metrics


def render_confusion_matrix(y_true: pd.Series, y_pred: pd.Series, labels: list[str]) -> None:
    """Render confusion matrix as a heatmap."""
    matrix = confusion_matrix(y_true, y_pred, labels=labels)
    fig, ax = plt.subplots(figsize=(5, 4))
    sns.heatmap(matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels, ax=ax)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")
    ax.set_title("Confusion Matrix")
    st.pyplot(fig)
    plt.close(fig)


def _format_feature_label(feature_name: str) -> str:
    """Turn raw feature names into more readable field labels."""
    return feature_name.replace("-", " ").replace("_", " ").strip().title()


def build_manual_input(metadata: Mapping[str, Any]) -> pd.DataFrame:
    """Build a single-row dataframe from user-provided feature inputs."""
    feature_columns: list[str] = list(metadata["feature_columns"])
    defaults: Mapping[str, Any] = metadata["default_values"]
    ui_hints: Mapping[str, Any] = metadata.get("ui_hints", {})
    categorical_options: Mapping[str, list[str]] = ui_hints.get("categorical_options", {})

    collected: dict[str, Any] = {}
    for feature_name in feature_columns:
        default_value = defaults.get(feature_name, "")
        label = _format_feature_label(feature_name)

        options = categorical_options.get(feature_name)
        if isinstance(options, list) and len(options) >= 2:
            safe_default = str(default_value) if str(default_value) in options else options[0]
            collected[feature_name] = st.selectbox(label, options=options, index=options.index(safe_default))
            continue

        if isinstance(default_value, (int, float)):
            collected[feature_name] = st.number_input(label, value=float(default_value))
        else:
            collected[feature_name] = st.text_input(label, value=str(default_value))

    return pd.DataFrame([collected])


def build_predictions_table(
    input_df: pd.DataFrame,
    predicted_labels: pd.Series,
    positive_probabilities: pd.Series,
    predicted_label_column: str,
    probability_column: str,
) -> pd.DataFrame:
    """Join predictions back onto the original data for display/export."""
    output_df = input_df.copy()
    output_df[predicted_label_column] = predicted_labels.values
    output_df[probability_column] = positive_probabilities.values
    return output_df


def main() -> None:
    """Run Streamlit interface."""
    st.set_page_config(page_title=APP_TITLE, layout="wide", page_icon="ðŸ“ˆ")
    apply_global_styles()

    st.markdown(f"## {APP_TITLE}")
    st.markdown(
        '<div class="muted tiny">Compare benchmark scores, upload a CSV for batch scoring, or try a single manual example.</div>',
        unsafe_allow_html=True,
    )

    if not MODEL_DIR.exists():
        st.error("Model artifacts not found.")
        st.code("python train_models.py")
        return

    try:
        metadata = load_metadata()
    except FileNotFoundError:
        st.error("Missing `model/metadata.json`. Train models first.")
        st.code("python train_models.py")
        return

    with st.spinner("Loading model pipelines..."):
        models = load_all_models()

    if not models:
        st.error("No trained models found in `model/`.")
        st.code("python train_models.py")
        return

    st.sidebar.markdown("### Controls")
    model_names = sorted(models.keys())
    selected_model_name = st.sidebar.selectbox("Model", options=model_names)
    threshold = st.sidebar.slider("Decision threshold", 0.05, 0.95, 0.50, 0.05)
    # Use checkbox for broad Streamlit compatibility.
    show_advanced = st.sidebar.checkbox("Show advanced outputs", value=False)

    selected_model = models[selected_model_name]
    positive_label = metadata["positive_label"]
    class_labels = metadata["class_labels"]
    negative_label = class_labels[0]
    target_column = metadata["target_column"]
    feature_columns: list[str] = list(metadata["feature_columns"])

    st.markdown(
        f"""
<div class="card">
  <div><b>Selected model</b>: {selected_model_name}</div>
  <div class="muted tiny">Threshold {threshold:.2f} â†’ predict <b>{positive_label}</b> when probability â‰¥ threshold.</div>
</div>
""",
        unsafe_allow_html=True,
    )

    tab_leaderboard, tab_batch, tab_single, tab_reference, tab_about = st.tabs(
        ["Model leaderboard", "Batch scoring", "Single prediction", "Reference evaluation", "About"]
    )

    with tab_leaderboard:
        st.markdown("### Saved benchmark metrics (held-out test split)")
        comparison_path = MODEL_DIR / "model_comparison.csv"
        if comparison_path.exists():
            score_df = pd.read_csv(comparison_path)
            st.dataframe(score_df, width="stretch", hide_index=True)
            st.download_button(
                "Download leaderboard CSV",
                data=score_df.to_csv(index=False).encode("utf-8"),
                file_name="model_comparison.csv",
                mime="text/csv",
            )
        else:
            st.warning("Benchmark file `model/model_comparison.csv` not found. Re-run training to regenerate it.")

        if show_advanced:
            st.markdown("#### Training metadata (advanced)")
            st.json(dict(metadata))

    with tab_batch:
        st.markdown("### Upload a CSV for batch scoring")
        st.caption(
            f"Your CSV must include all feature columns. If it also includes `{target_column}`, the app will compute metrics."
        )

        uploaded_file = st.file_uploader("CSV file", type=["csv"])
        if uploaded_file is None:
            st.info("Upload a CSV to generate predictions.")
        else:
            uploaded_df = pd.read_csv(uploaded_file)
            missing_features = sorted(set(feature_columns) - set(uploaded_df.columns))
            if missing_features:
                st.error("Missing required feature columns.")
                st.write(missing_features)
            else:
                features_df = uploaded_df[feature_columns].copy()
                positive_probabilities = pd.Series(selected_model.predict_proba(features_df)[:, 1], name="p_positive")
                predicted_labels = pd.Series(
                    [positive_label if p >= threshold else negative_label for p in positive_probabilities],
                    name="prediction",
                )

                result_df = build_predictions_table(
                    input_df=uploaded_df,
                    predicted_labels=predicted_labels,
                    positive_probabilities=positive_probabilities,
                    predicted_label_column="predicted_income",
                    probability_column=f"probability_{positive_label.replace('>','gt_').replace('<=','le_')}",
                )

                st.markdown("#### Preview")
                st.dataframe(result_df.head(25), width="stretch", hide_index=True)
                st.download_button(
                    "Download predictions CSV",
                    data=result_df.to_csv(index=False).encode("utf-8"),
                    file_name="predictions.csv",
                    mime="text/csv",
                )

                if target_column in uploaded_df.columns:
                    y_true = uploaded_df[target_column].astype(str).str.strip()
                    metrics = compute_metrics(y_true, predicted_labels, positive_probabilities, positive_label)
                    metric_cols = st.columns(6)
                    metric_cols[0].metric("Accuracy", f"{metrics['Accuracy']:.4f}")
                    metric_cols[1].metric("AUC", f"{metrics['AUC']:.4f}")
                    metric_cols[2].metric("Precision", f"{metrics['Precision']:.4f}")
                    metric_cols[3].metric("Recall", f"{metrics['Recall']:.4f}")
                    metric_cols[4].metric("F1", f"{metrics['F1 Score']:.4f}")
                    metric_cols[5].metric("MCC", f"{metrics['MCC']:.4f}")

                    st.markdown("#### Confusion matrix")
                    render_confusion_matrix(y_true, predicted_labels, class_labels)

                    if show_advanced:
                        st.markdown("#### Classification report (advanced)")
                        report = classification_report(y_true, predicted_labels, output_dict=True)
                        st.dataframe(pd.DataFrame(report).transpose(), width="stretch")
                else:
                    st.info(f"Column `{target_column}` not found, so only predictions are shown.")

    with tab_single:
        st.markdown("### Single manual prediction")
        st.caption("Fill feature values and run one prediction through the selected pipeline.")

        with st.form("manual_prediction_form"):
            manual_input_df = build_manual_input(metadata)
            submitted = st.form_submit_button("Predict")

        if submitted:
            positive_probability = float(selected_model.predict_proba(manual_input_df)[:, 1][0])
            predicted_label = positive_label if positive_probability >= threshold else negative_label

            left, right = st.columns([1, 2])
            left.metric("Predicted label", predicted_label)
            left.metric(f"P({positive_label})", f"{positive_probability:.4f}")
            right.dataframe(manual_input_df, width="stretch", hide_index=True)

    with tab_reference:
        st.markdown("### Reference test-set evaluation")
        st.caption("Uses `data/test_reference.csv` created by the training script.")

        reference_path = DATA_DIR / "test_reference.csv"
        if not reference_path.exists():
            st.info("Reference test set not found. Train models to generate it.")
            st.code("python train_models.py")
        else:
            reference_df = pd.read_csv(reference_path)
            x_ref = reference_df[feature_columns]
            y_ref = reference_df[target_column].astype(str).str.strip()
            positive_probabilities = pd.Series(selected_model.predict_proba(x_ref)[:, 1], name="p_positive")
            y_pred = pd.Series(
                [positive_label if p >= threshold else negative_label for p in positive_probabilities],
                name="prediction",
            )
            ref_metrics = compute_metrics(y_ref, y_pred, positive_probabilities, positive_label)

            metric_cols = st.columns(6)
            metric_cols[0].metric("Accuracy", f"{ref_metrics['Accuracy']:.4f}")
            metric_cols[1].metric("AUC", f"{ref_metrics['AUC']:.4f}")
            metric_cols[2].metric("Precision", f"{ref_metrics['Precision']:.4f}")
            metric_cols[3].metric("Recall", f"{ref_metrics['Recall']:.4f}")
            metric_cols[4].metric("F1", f"{ref_metrics['F1 Score']:.4f}")
            metric_cols[5].metric("MCC", f"{ref_metrics['MCC']:.4f}")

            if show_advanced:
                st.markdown("#### Confusion matrix (advanced)")
                render_confusion_matrix(y_ref, y_pred, class_labels)

    with tab_about:
        st.markdown("### About")
        st.markdown(
            """
- **Dataset**: Adult Income (OpenML)
- **Target**: income (`<=50K` vs `>50K`)
- **Artifacts**: pipelines in `model/`, reference split in `data/`
""".strip()
        )


if __name__ == "__main__":
    main()
